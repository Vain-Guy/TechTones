{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bb00217",
   "metadata": {},
   "source": [
    "### ***AUTHORS***\n",
    "1. Kiprono Ben\n",
    "2. Norman Mwapea\n",
    "3. Pauline Kariuki\n",
    "4. Wesley Owino\n",
    "5. Judith Otieno\n",
    "6. Alvin Kipleting\n",
    "\n",
    "## BUSINESS UNDERSTANDING\n",
    "\n",
    "### OVERVIEW\n",
    "\n",
    "This is a Natural Language Processing (NLP)-driven sentiment analysis project designed to decode public opinion on two of the world’s most influential tech companies; **Apple** and **Google**. By leveraging machine learning techniques to analyze thousands of real tweets, the project aims to classify user sentiment as *positive*, *negative*, or *neutral*.  \n",
    "\n",
    "This initiative demonstrates how social media analytics can provide actionable insights for companies seeking to understand consumer perceptions, monitor brand reputation, and anticipate market trends. Ultimately, the project serves as a prototype for a scalable, intelligent sentiment monitoring system applicable across industries.\n",
    "\n",
    "### BACKGROUND\n",
    "\n",
    "Social media has become the world’s largest real-time feedback loop. Millions of users share their thoughts daily about products, services, and brands; creating a goldmine of unstructured data that reveals how people truly feel.  \n",
    "For technology companies like **Apple** and **Google**, such sentiment can directly influence *brand equity*, *purchase behavior*, and *public trust*. Tweets praising a product’s innovation or criticizing a software update can ripple across digital spaces, shaping collective opinion within hours.  \n",
    "Understanding these emotional currents is therefore critical for modern businesses. Sentiment analysis transforms raw textual chatter into measurable, strategic insight—empowering companies to react faster, market smarter, and communicate better.\n",
    "\n",
    "### OBJECTIVES\n",
    "\n",
    "The main goals of the project are to:\n",
    "\n",
    "1. Develop a Natural Language Processing (NLP) model that classifies tweets related to Apple and Google as *positive*, *negative*, or *neutral*.  \n",
    "2. Preprocess and transform raw text into machine-readable features using tokenization, normalization, stopword removal, and TF-IDF vectorization.  \n",
    "3. Evaluate multiple machine learning algorithms to identify the most accurate and interpretable classifier.  \n",
    "4. Generate data-driven insights about brand sentiment patterns to support business and marketing decisions.  \n",
    "5. Lay the groundwork for an automated brand intelligence system capable of tracking sentiment across multiple platforms and industries.\n",
    "\n",
    "### SUCCESS METRICS\n",
    "\n",
    "Success will be defined through a mix of technical and business outcomes:\n",
    "\n",
    "- **Model Performance:** Achieving at least 80% F1-score across sentiment classes.  \n",
    "- **Data Integrity:** Clean, balanced, and reproducible dataset suitable for future extensions.  \n",
    "- **Interpretability:** Ability to visualize top sentiment-driving words and phrases for explainability.  \n",
    "- **Insight Quality:** Sentiment trends that clearly reflect real-world brand perceptions.  \n",
    "- **Scalability:** Modular design allowing future integration with streaming APIs for live monitoring.\n",
    "\n",
    "### STAKEHOLDERS\n",
    "\n",
    "The beneficiaries of this undertaking include:\n",
    "  \n",
    "- **Marketing Analysts:** Use sentiment insights to understand public opinion and campaign impact.  \n",
    "- **Product Managers:** Monitor consumer feedback and sentiment shifts after product launches.  \n",
    "- **Executives & Decision Makers:** Leverage findings to inform strategic brand and communication strategies.\n",
    "- **Data Science & NLP Teams:** Responsible for model design, feature engineering, and evaluation.  \n",
    "- **Research & Development Teams:** Explore applications of the model in broader domains like product reviews, customer feedback, and crisis management.\n",
    "\n",
    "#### RELEVANCE\n",
    "\n",
    "TechTones bridges the gap between data and perception. It shows how machine learning can turn the chaos of social media into structured intelligence; helping organizations not only track how people feel, but also why they feel that way.\n",
    "This project highlights the growing role of NLP in business strategy, reputation management, and competitive intelligence, providing a strong proof of concept for sentiment analysis as a key driver of modern data-driven decision-making.\n",
    "\n",
    "## DATA UNDERSTANDING\n",
    " \n",
    "The dataset used in this project is sourced from [CrowdFlower](https://data.world/crowdflower/brands-and-product-emotions) and contains over 9,000 Tweets referencing Apple and Google products. Each Tweet has been annotated with information identifying the product or brand mentioned and the emotion expressed toward it. It offers a real-world foundation for supervised sentiment analysis in Natural Language Processing (NLP). \n",
    "\n",
    "It contains the following features with their descriptions:\n",
    "\n",
    "| Feature | Description |\n",
    "|--------------|----------------|\n",
    "| tweet_text | The full text of each Tweet referencing Apple or Google products. |\n",
    "| emotion_in_tweet_is_directed_at | The product or brand mentioned (e.g iPhone, iPad, Google, iPad/iPhone App). |\n",
    "| is_there_an_emotion_directed_at_a_brand_or_product | The annotated sentiment label -> *Positive emotion*, *Negative emotion*, or *No emotion toward brand or product*. |\n",
    "\n",
    "This schema supports a supervised learning setup where tweet_text acts as the input feature and is_there_an_emotion_directed_at_a_brand_or_product serves as the target variable for classification.\n",
    "\n",
    "Awesome! Now, let's get our hands dirty.\n",
    "\n",
    "### EXPLORING THE DATA\n",
    "\n",
    "We begin by loading the data and performing an initial exploration to get a sense of its structure and content. But first, we gather our essential tools; just as a painter readies their palette and brushes before creating a masterpiece."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "48e14c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# ======= [Import all relevant libraries] =======\n",
    "\n",
    "# Utilities\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Usual Suspects\n",
    "import numpy as np           # Mathematical operations\n",
    "import pandas as pd          # Data manipulation\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "import seaborn as sns\n",
    "\n",
    "# String manipulation\n",
    "import re\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer                   # Tokenization\n",
    "from nltk.corpus import stopwords                           # Stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer      # Stemming & Lemmatization\n",
    "\n",
    "# Vectorization\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Pipelines\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# ML Models\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression         # Logistic Regression\n",
    "from sklearn.tree import DecisionTreeClassifier             # Decision Tree\n",
    "\n",
    "# ML Model Evaluation\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    ConfusionMatrixDisplay, confusion_matrix,\n",
    "    roc_curve, auc,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "# Set column display to maximum\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a77ba25",
   "metadata": {},
   "source": [
    "Now we load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "b7565113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/iPhone app that you'll likely appreciate for its design. Also, they're giving free Ts at #SXSW</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. They should sale them down at #SXSW.</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as crashy as this year's iPhone app. #sxsw</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa Mayer (Google), Tim O'Reilly (tech books/conferences) &amp;amp; Matt Mullenweg (Wordpress)</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9088</th>\n",
       "      <td>Ipad everywhere. #SXSW {link}</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9089</th>\n",
       "      <td>Wave, buzz... RT @mention We interrupt your regularly scheduled #sxsw geek programming with big news {link}  #google #circles</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9090</th>\n",
       "      <td>Google's Zeiger, a physician never reported potential AE. Yet FDA relies on physicians. &amp;quot;We're operating w/out data.&amp;quot; #sxsw #health2dev</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9091</th>\n",
       "      <td>Some Verizon iPhone customers complained their time fell back an hour this weekend.  Of course they were the New Yorkers who attended #SXSW.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9092</th>\n",
       "      <td>Ï¡Ïàü_ÊÎÒ£Áââ_£â_ÛâRT @mention Google Tests ÛÏCheck-in OffersÛ At #SXSW {link}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9093 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                             tweet_text  \\\n",
       "0                       .@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.   \n",
       "1           @jessedee Know about @fludapp ? Awesome iPad/iPhone app that you'll likely appreciate for its design. Also, they're giving free Ts at #SXSW   \n",
       "2                                                                       @swonderlin Can not wait for #iPad 2 also. They should sale them down at #SXSW.   \n",
       "3                                                                    @sxsw I hope this year's festival isn't as crashy as this year's iPhone app. #sxsw   \n",
       "4                   @sxtxstate great stuff on Fri #SXSW: Marissa Mayer (Google), Tim O'Reilly (tech books/conferences) &amp; Matt Mullenweg (Wordpress)   \n",
       "...                                                                                                                                                 ...   \n",
       "9088                                                                                                                      Ipad everywhere. #SXSW {link}   \n",
       "9089                      Wave, buzz... RT @mention We interrupt your regularly scheduled #sxsw geek programming with big news {link}  #google #circles   \n",
       "9090  Google's Zeiger, a physician never reported potential AE. Yet FDA relies on physicians. &quot;We're operating w/out data.&quot; #sxsw #health2dev   \n",
       "9091       Some Verizon iPhone customers complained their time fell back an hour this weekend.  Of course they were the New Yorkers who attended #SXSW.   \n",
       "9092                                           Ï¡Ïàü_ÊÎÒ£Áââ_£â_ÛâRT @mention Google Tests ÛÏCheck-in OffersÛ At #SXSW {link}   \n",
       "\n",
       "     emotion_in_tweet_is_directed_at  \\\n",
       "0                             iPhone   \n",
       "1                 iPad or iPhone App   \n",
       "2                               iPad   \n",
       "3                 iPad or iPhone App   \n",
       "4                             Google   \n",
       "...                              ...   \n",
       "9088                            iPad   \n",
       "9089                             NaN   \n",
       "9090                             NaN   \n",
       "9091                             NaN   \n",
       "9092                             NaN   \n",
       "\n",
       "     is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                      Negative emotion  \n",
       "1                                      Positive emotion  \n",
       "2                                      Positive emotion  \n",
       "3                                      Negative emotion  \n",
       "4                                      Positive emotion  \n",
       "...                                                 ...  \n",
       "9088                                   Positive emotion  \n",
       "9089                 No emotion toward brand or product  \n",
       "9090                 No emotion toward brand or product  \n",
       "9091                 No emotion toward brand or product  \n",
       "9092                 No emotion toward brand or product  \n",
       "\n",
       "[9093 rows x 3 columns]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"C:\\\\Users\\\\lenovo\\\\OneDrive\\\\Desktop\\\\DS\\\\PROJECTS\\\\TechTones\\\\Apple and Google Twitter Sentiments.csv\", encoding=\"ISO-8859-1\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2404f8c",
   "metadata": {},
   "source": [
    "*Observation:* The dataset is consistent and admittedly very messy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea474939",
   "metadata": {},
   "source": [
    "Then we check the number of records and fatures we are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "dbc7af0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 9093 records and 3 features.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The dataset has {df.shape[0]} records and {df.shape[1]} features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967cdfa5",
   "metadata": {},
   "source": [
    "We’ve got over 9,000 records, a solid data haul. Let’s see how unique the dataset is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "73e29953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique values in: \n",
      "\n",
      "tweet_text: 9065 unique values\n",
      "emotion_in_tweet_is_directed_at: 9 unique values\n",
      "is_there_an_emotion_directed_at_a_brand_or_product: 4 unique values\n",
      "\n",
      "Unique Values in:\n",
      "\n",
      "tweet_text:\n",
      "['.@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.'\n",
      " \"@jessedee Know about @fludapp ? Awesome iPad/iPhone app that you'll likely appreciate for its design. Also, they're giving free Ts at #SXSW\"\n",
      " '@swonderlin Can not wait for #iPad 2 also. They should sale them down at #SXSW.'\n",
      " ...\n",
      " \"Google's Zeiger, a physician never reported potential AE. Yet FDA relies on physicians. &quot;We're operating w/out data.&quot; #sxsw #health2dev\"\n",
      " 'Some Verizon iPhone customers complained their time fell back an hour this weekend.  Of course they were the New Yorkers who attended #SXSW.'\n",
      " '\\x8cÏ¡\\x8eÏà\\x8aü_\\x8b\\x81Ê\\x8b\\x81Î\\x8b\\x81Ò\\x8b\\x81£\\x8b\\x81Á\\x8bââ\\x8b\\x81_\\x8b\\x81£\\x8b\\x81\\x8f\\x8bâ_\\x8bÛâRT @mention Google Tests \\x89ÛÏCheck-in Offers\\x89Û\\x9d At #SXSW {link}']\n",
      "\n",
      "emotion_in_tweet_is_directed_at:\n",
      "['iPhone' 'iPad or iPhone App' 'iPad' 'Google' nan 'Android' 'Apple'\n",
      " 'Android App' 'Other Google product or service'\n",
      " 'Other Apple product or service']\n",
      "\n",
      "is_there_an_emotion_directed_at_a_brand_or_product:\n",
      "['Negative emotion' 'Positive emotion'\n",
      " 'No emotion toward brand or product' \"I can't tell\"]\n"
     ]
    }
   ],
   "source": [
    "# ======= [Dataset Uniqueness] =======\n",
    "\n",
    "# Number of unique values in each column\n",
    "print(\"Number of unique values in:\", '\\n')\n",
    "for col in df.columns:\n",
    "    print(f\"{col}: {df[col].nunique()} unique values\")\n",
    "\n",
    "# Unique values in each column\n",
    "print(\"\\nUnique Values in:\")\n",
    "for col in df.columns:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(df[col].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfbf1c9",
   "metadata": {},
   "source": [
    "*Observation:*\n",
    "\n",
    "- There are over 9,065 unique tweet entries, meaning nearly every tweet in the dataset is distinct.\n",
    "- These tweets are directed to 9 unique products, capturing a range of Apple and Google products (like iPhone, Android, iPad) and some general or unspecified mentions. The presence of NaN values suggests that some tweets don’t explicitly mention a product.\n",
    "- There are 4 distinct emotional classifications, reflecting the sentiment intensity or clarity.\n",
    "\n",
    "Based on this initial preview, it makes sense to standardize the column names; keeping them short, clear, and code friendly. We’ll rename them as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "f97f0b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tweet', 'product', 'sentiment'], dtype='object')"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename the columns\n",
    "df.rename(columns={\n",
    "    'tweet_text': 'tweet',\n",
    "    'emotion_in_tweet_is_directed_at': 'product',\n",
    "    'is_there_an_emotion_directed_at_a_brand_or_product': 'sentiment'\n",
    "}, inplace=True)\n",
    "\n",
    "# Preview new column names\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7236cfd6",
   "metadata": {},
   "source": [
    "We further check the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "53411486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9093 entries, 0 to 9092\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   tweet      9092 non-null   object\n",
      " 1   product    3291 non-null   object\n",
      " 2   sentiment  9093 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 213.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bedb6d",
   "metadata": {},
   "source": [
    "*Interpretation:*\n",
    "\n",
    "- All fields are categorical which is consistent with the data as it is in text format.\n",
    "- tweet: Almost complete, with 9,092 non-null values, meaning only one missing entry.\n",
    "- product: Has 3,291 non-null values, showing that about 36% of tweets mention a specific Apple or Google product. The rest are either general statements or lack a clear product reference.\n",
    "- sentiment: Fully populated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1177d4b",
   "metadata": {},
   "source": [
    "Next, we aim to deepen our understanding of the dataset by exploring the descriptive statistics of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "6268eb2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tweet</th>\n",
       "      <td>9092</td>\n",
       "      <td>9065</td>\n",
       "      <td>RT @mention Marissa Mayer: Google Will Connect the Digital &amp;amp; Physical Worlds Through Mobile - {link} #sxsw</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product</th>\n",
       "      <td>3291</td>\n",
       "      <td>9</td>\n",
       "      <td>iPad</td>\n",
       "      <td>946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <td>9093</td>\n",
       "      <td>4</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>5389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          count unique  \\\n",
       "tweet      9092   9065   \n",
       "product    3291      9   \n",
       "sentiment  9093      4   \n",
       "\n",
       "                                                                                                                      top  \\\n",
       "tweet      RT @mention Marissa Mayer: Google Will Connect the Digital &amp; Physical Worlds Through Mobile - {link} #sxsw   \n",
       "product                                                                                                              iPad   \n",
       "sentiment                                                                              No emotion toward brand or product   \n",
       "\n",
       "           freq  \n",
       "tweet         5  \n",
       "product     946  \n",
       "sentiment  5389  "
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(include='O').T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbee4cd",
   "metadata": {},
   "source": [
    "*Observation:*\n",
    "\n",
    "We observe that:\n",
    "\n",
    "- The most repeated tweet appears only 5 times, suggesting very little duplication.\n",
    "- Products (product): Out of all the 9 distinct products, iPads lead the conversation -> mentioned 946 times, indicating a strong public interest in them.\n",
    "- The dataset is dominated by neutral or indifferent opinions -> 'No emotion toward brand or product' appears 5,389 times, making up more than half the data. This suggests that while people talk about these brands a lot, many tweets don’t clearly express positive or negative emotions -> neutrality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c87f5c5",
   "metadata": {},
   "source": [
    "### DATA QUALITY CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "37732fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates: 22\n",
      "\n",
      "Missing values and percentage missingness:\n",
      "            Missing Values  Percentage\n",
      "tweet                   1    0.010997\n",
      "product              5802   63.807324\n",
      "sentiment               0    0.000000\n"
     ]
    }
   ],
   "source": [
    "# ======= [Check for duplicates and missing values] =======\n",
    "\n",
    "# Check for duplicates\n",
    "print(\"Duplicates:\", df.duplicated().sum())\n",
    "\n",
    "# Check for missing values and missingness percentage\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "missing_info = pd.DataFrame({'Missing Values': missing_values, 'Percentage': missing_percentage})\n",
    "print(\"\\nMissing values and percentage missingness:\\n\", missing_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e435acda",
   "metadata": {},
   "source": [
    "*Comment on data quality:*\n",
    "\n",
    "Yikes! The product column has more than half of its values missing; we’ll address that appropriately. There are also 22 duplicate records, which we’ll remove, along with any rows containing null tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f66c90d",
   "metadata": {},
   "source": [
    "# DATA CLEANING AND PREPARATION\n",
    "\n",
    "To enable meaningful analysis, we must first clean and prepare the data through the following cleaning and preprocessing steps:\n",
    "\n",
    "#### 1. Drop duplicate rows and nulls only in tweet columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "d71f7735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates: 0\n",
      "\n",
      "Missing values:\n",
      "            Missing Values\n",
      "tweet                   0\n",
      "product              5788\n",
      "sentiment               0\n"
     ]
    }
   ],
   "source": [
    "# Drop duplicates and nulls \n",
    "df = df.drop_duplicates().dropna(subset=['tweet'])\n",
    "\n",
    "# Sanity check\n",
    "# Re-check for duplicates\n",
    "print(\"Duplicates:\", df.duplicated().sum())\n",
    "\n",
    "# Re-check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "missing_info = pd.DataFrame({'Missing Values': missing_values})\n",
    "print(\"\\nMissing values:\\n\", missing_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5317a0d9",
   "metadata": {},
   "source": [
    "#### 2. Dealing missing values in product column\n",
    "\n",
    "We begin by inspecting the null values in the product column in relation to the tweet content to determine whether any clues about the missing product can be inferred from the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "e855fa3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@teachntech00 New iPad Apps For #SpeechTherapy And Communication Are Showcased At The #SXSW Conference http://ht.ly/49n4M #iear #edchat #asd</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Holler Gram for iPad on the iTunes App Store -  http://t.co/kfN3f5Q (via @marc_is_ken) #sxsw</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Attn: All  #SXSW frineds, @mention Register for #GDGTLive  and see Cobra iRadar for Android. {link}</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Anyone at  #sxsw want to sell their old iPad?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Anyone at  #SXSW who bought the new iPad want to sell their older iPad to me?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9087</th>\n",
       "      <td>@mention Yup, but I don't have a third app yet. I'm on Android, any suggestions? #SXSW CC: @mention</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9089</th>\n",
       "      <td>Wave, buzz... RT @mention We interrupt your regularly scheduled #sxsw geek programming with big news {link}  #google #circles</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9090</th>\n",
       "      <td>Google's Zeiger, a physician never reported potential AE. Yet FDA relies on physicians. &amp;quot;We're operating w/out data.&amp;quot; #sxsw #health2dev</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9091</th>\n",
       "      <td>Some Verizon iPhone customers complained their time fell back an hour this weekend.  Of course they were the New Yorkers who attended #SXSW.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9092</th>\n",
       "      <td>Ï¡Ïàü_ÊÎÒ£Áââ_£â_ÛâRT @mention Google Tests ÛÏCheck-in OffersÛ At #SXSW {link}</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5788 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                  tweet  \\\n",
       "5          @teachntech00 New iPad Apps For #SpeechTherapy And Communication Are Showcased At The #SXSW Conference http://ht.ly/49n4M #iear #edchat #asd   \n",
       "16                                                         Holler Gram for iPad on the iTunes App Store -  http://t.co/kfN3f5Q (via @marc_is_ken) #sxsw   \n",
       "32                                                  Attn: All  #SXSW frineds, @mention Register for #GDGTLive  and see Cobra iRadar for Android. {link}   \n",
       "33                                                                                                        Anyone at  #sxsw want to sell their old iPad?   \n",
       "34                                                                        Anyone at  #SXSW who bought the new iPad want to sell their older iPad to me?   \n",
       "...                                                                                                                                                 ...   \n",
       "9087                                                @mention Yup, but I don't have a third app yet. I'm on Android, any suggestions? #SXSW CC: @mention   \n",
       "9089                      Wave, buzz... RT @mention We interrupt your regularly scheduled #sxsw geek programming with big news {link}  #google #circles   \n",
       "9090  Google's Zeiger, a physician never reported potential AE. Yet FDA relies on physicians. &quot;We're operating w/out data.&quot; #sxsw #health2dev   \n",
       "9091       Some Verizon iPhone customers complained their time fell back an hour this weekend.  Of course they were the New Yorkers who attended #SXSW.   \n",
       "9092                                           Ï¡Ïàü_ÊÎÒ£Áââ_£â_ÛâRT @mention Google Tests ÛÏCheck-in OffersÛ At #SXSW {link}   \n",
       "\n",
       "     product  \n",
       "5        NaN  \n",
       "16       NaN  \n",
       "32       NaN  \n",
       "33       NaN  \n",
       "34       NaN  \n",
       "...      ...  \n",
       "9087     NaN  \n",
       "9089     NaN  \n",
       "9090     NaN  \n",
       "9091     NaN  \n",
       "9092     NaN  \n",
       "\n",
       "[5788 rows x 2 columns]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine tweets with missing product values \n",
    "# to identify potential product mentions within the text\n",
    "df[df['product'].isna()][['tweet', 'product']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ceabcf3",
   "metadata": {},
   "source": [
    "From the inspection, it’s evident that several tweets with missing product values actually contain clues about the product within the text itself. However, the current tweet column is too noisy to analyze effectively. It needs cleaning and preprocessing first.\n",
    "\n",
    "After preprocessing, we’ll create a mapping strategy to standardize product mentions:\n",
    "- Any Apple-related terms (e.g iPhone, iPad) will be grouped under Apple.\n",
    "- All other identifiable products will be categorized under Google."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a27241e",
   "metadata": {},
   "source": [
    "## PREPROCESSING STEPS:\n",
    "\n",
    "1. Convert all tweets to lowercase for uniformity.\n",
    "2. Remove stopwords to eliminate irrelevant filler words.\n",
    "3. Strip punctuation and special characters.\n",
    "4. Tokenize the text to break it into analyzable units.\n",
    "5. Apply lemmatization to reduce words to their root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "4c5405b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>product</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokens</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8505</th>\n",
       "      <td>Well @mention @mention Apple is opening up a temporary store in downtown Austin for #SXSW and the iPad 2 launch {link}</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>[well, apple, opening, temporary, store, downtown, austin, sxsw, ipad, launch, link]</td>\n",
       "      <td>well apple opening temporary store downtown austin sxsw ipad launch link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7556</th>\n",
       "      <td>Free 22 track #sxsw sampler album on iTunes. #music #discovery {link}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>[free, track, sxsw, sampler, album, itunes, music, discovery, link]</td>\n",
       "      <td>free track sxsw sampler album itunes music discovery link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8899</th>\n",
       "      <td>Deciding when to release a product is an art, not a science - @mention at @mention #sxsw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>[deciding, release, product, art, science, sxsw]</td>\n",
       "      <td>deciding release product art science sxsw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2702</th>\n",
       "      <td>1/4 of the people in line for the iP*d 2 #SXSW (@mention Apple Store, SXSW) {link}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>[people, line, sxsw, apple, store, sxsw, link]</td>\n",
       "      <td>people line sxsw apple store sxsw link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3021</th>\n",
       "      <td>Apple Offers Up Free iTunes Sampler Ahead of #SXSW to feature new artists {link} via @mention #music</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>[apple, offer, free, itunes, sampler, ahead, sxsw, feature, new, artist, link, via, music]</td>\n",
       "      <td>apple offer free itunes sampler ahead sxsw feature new artist link via music</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337</th>\n",
       "      <td>Wow. Just witnessed a homeless guy giving detailed ipad 2 inventory info to a passerby at the Austin popup store. #SXSW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>[wow, witnessed, homeless, guy, giving, detailed, ipad, inventory, info, passerby, austin, popup, store, sxsw]</td>\n",
       "      <td>wow witnessed homeless guy giving detailed ipad inventory info passerby austin popup store sxsw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5323</th>\n",
       "      <td>RT @mention #SXSW Why Google, Apple, EA Games, Zynga, Facebook, Microsoft and Intuit went gaga for The Go Game: {link}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No emotion toward brand or product</td>\n",
       "      <td>[sxsw, google, apple, game, zynga, facebook, microsoft, intuit, went, gaga, game, link]</td>\n",
       "      <td>sxsw google apple game zynga facebook microsoft intuit went gaga game link</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                        tweet  \\\n",
       "8505   Well @mention @mention Apple is opening up a temporary store in downtown Austin for #SXSW and the iPad 2 launch {link}   \n",
       "7556                                                    Free 22 track #sxsw sampler album on iTunes. #music #discovery {link}   \n",
       "8899                                 Deciding when to release a product is an art, not a science - @mention at @mention #sxsw   \n",
       "2702                                       1/4 of the people in line for the iP*d 2 #SXSW (@mention Apple Store, SXSW) {link}   \n",
       "3021                     Apple Offers Up Free iTunes Sampler Ahead of #SXSW to feature new artists {link} via @mention #music   \n",
       "1337  Wow. Just witnessed a homeless guy giving detailed ipad 2 inventory info to a passerby at the Austin popup store. #SXSW   \n",
       "5323   RT @mention #SXSW Why Google, Apple, EA Games, Zynga, Facebook, Microsoft and Intuit went gaga for The Go Game: {link}   \n",
       "\n",
       "     product                           sentiment  \\\n",
       "8505   Apple                    Positive emotion   \n",
       "7556     NaN  No emotion toward brand or product   \n",
       "8899     NaN  No emotion toward brand or product   \n",
       "2702     NaN  No emotion toward brand or product   \n",
       "3021     NaN  No emotion toward brand or product   \n",
       "1337     NaN                    Positive emotion   \n",
       "5323     NaN  No emotion toward brand or product   \n",
       "\n",
       "                                                                                                              tokens  \\\n",
       "8505                            [well, apple, opening, temporary, store, downtown, austin, sxsw, ipad, launch, link]   \n",
       "7556                                             [free, track, sxsw, sampler, album, itunes, music, discovery, link]   \n",
       "8899                                                                [deciding, release, product, art, science, sxsw]   \n",
       "2702                                                                  [people, line, sxsw, apple, store, sxsw, link]   \n",
       "3021                      [apple, offer, free, itunes, sampler, ahead, sxsw, feature, new, artist, link, via, music]   \n",
       "1337  [wow, witnessed, homeless, guy, giving, detailed, ipad, inventory, info, passerby, austin, popup, store, sxsw]   \n",
       "5323                         [sxsw, google, apple, game, zynga, facebook, microsoft, intuit, went, gaga, game, link]   \n",
       "\n",
       "                                                                                          clean_tweet  \n",
       "8505                         well apple opening temporary store downtown austin sxsw ipad launch link  \n",
       "7556                                        free track sxsw sampler album itunes music discovery link  \n",
       "8899                                                        deciding release product art science sxsw  \n",
       "2702                                                           people line sxsw apple store sxsw link  \n",
       "3021                     apple offer free itunes sampler ahead sxsw feature new artist link via music  \n",
       "1337  wow witnessed homeless guy giving detailed ipad inventory info passerby austin popup store sxsw  \n",
       "5323                       sxsw google apple game zynga facebook microsoft intuit went gaga game link  "
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ======= [Text Preprocessing Pipeline] =======\n",
    "\n",
    "# Initialize tools\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stopwords_list = set(stopwords.words('english'))  \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def janitor(text):\n",
    "\n",
    "    '''\n",
    "    Cleans and preprocesses tweet text for analysis.\n",
    "\n",
    "    Steps:\n",
    "        1. Remove URLs, mentions, and hashtags\n",
    "        2. Lowercase text and strip whitespace\n",
    "        3. Tokenize\n",
    "        4. Remove stopwords and very short tokens\n",
    "        5. Lemmatize words\n",
    "    '''\n",
    "\n",
    "    # 1. Remove unwanted elements\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)  # URLs\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)                     # Mentions\n",
    "    text = re.sub(r\"#\", \"\", text)                        # Hashtags symbol only\n",
    "    text = text.lower().strip()\n",
    "\n",
    "    # 2. Tokenize\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    # 3. Filter stopwords and short tokens\n",
    "    tokens = [word for word in tokens if word not in stopwords_list and len(word) > 2]\n",
    "\n",
    "    # 4. Lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# ======= [Apply to DataFrame] =======\n",
    "df['tokens'] = df['tweet'].astype(str).apply(janitor)\n",
    "df['clean_tweet'] = df['tokens'].apply(' '.join)\n",
    "\n",
    "# Preview changes\n",
    "df.sample(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c5f09f",
   "metadata": {},
   "source": [
    "Awesome! Now we map the products into just Apple and Google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "d7d80df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "product\n",
       "NaN       5788\n",
       "Apple     2404\n",
       "Google     878\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define mapping rules\n",
    "apple_products = [\n",
    "    'iPhone', 'iPad or iPhone App', 'iPad', \n",
    "    'Apple', 'Other Apple product or service'\n",
    "]\n",
    "\n",
    "google_products = [\n",
    "    'Google', 'Android', 'Android App', \n",
    "    'Other Google product or service'\n",
    "]\n",
    "\n",
    "# Map. Only assign if in the specific list, otherwise keep NaN\n",
    "df['product'] = df['product'].apply(\n",
    "    lambda x: 'Apple' if x in apple_products else ('Google' if x in google_products else np.nan)\n",
    ")\n",
    "\n",
    "# Check distribution after mapping\n",
    "df['product'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc44d33",
   "metadata": {},
   "source": [
    "Now that we have tokenized the tweets, we can leverage these tokens to infer product associations. By examining keywords in the tokens; such as 'iphone', 'ipad', 'android', and 'google', we can impute the missing product values for tweets that didn't have explicit product labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b742190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[new, ipad, apps, speechtherapy, communication, showcased, sxsw, conference, iear, edchat, asd]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[holler, gram, ipad, itunes, app, store, via, sxsw]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[attn, sxsw, frineds, register, gdgtlive, see, cobra, iradar, android, link]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[anyone, sxsw, want, sell, old, ipad]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[anyone, sxsw, bought, new, ipad, want, sell, older, ipad]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9087</th>\n",
       "      <td>[yup, third, app, yet, android, suggestion, sxsw]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9089</th>\n",
       "      <td>[wave, buzz, interrupt, regularly, scheduled, sxsw, geek, programming, big, news, link, google, circle]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9090</th>\n",
       "      <td>[google, zeiger, physician, never, reported, potential, yet, fda, relies, physician, quot, operating, data, quot, sxsw, health2dev]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9091</th>\n",
       "      <td>[verizon, iphone, customer, complained, time, fell, back, hour, weekend, course, new, yorkers, attended, sxsw]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9092</th>\n",
       "      <td>[ûârt, google, test, ûïcheck, offer, sxsw, link]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5788 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                   tokens  \\\n",
       "5                                         [new, ipad, apps, speechtherapy, communication, showcased, sxsw, conference, iear, edchat, asd]   \n",
       "16                                                                                    [holler, gram, ipad, itunes, app, store, via, sxsw]   \n",
       "32                                                           [attn, sxsw, frineds, register, gdgtlive, see, cobra, iradar, android, link]   \n",
       "33                                                                                                  [anyone, sxsw, want, sell, old, ipad]   \n",
       "34                                                                             [anyone, sxsw, bought, new, ipad, want, sell, older, ipad]   \n",
       "...                                                                                                                                   ...   \n",
       "9087                                                                                    [yup, third, app, yet, android, suggestion, sxsw]   \n",
       "9089                              [wave, buzz, interrupt, regularly, scheduled, sxsw, geek, programming, big, news, link, google, circle]   \n",
       "9090  [google, zeiger, physician, never, reported, potential, yet, fda, relies, physician, quot, operating, data, quot, sxsw, health2dev]   \n",
       "9091                       [verizon, iphone, customer, complained, time, fell, back, hour, weekend, course, new, yorkers, attended, sxsw]   \n",
       "9092                                                                                     [ûârt, google, test, ûïcheck, offer, sxsw, link]   \n",
       "\n",
       "     product  \n",
       "5        NaN  \n",
       "16       NaN  \n",
       "32       NaN  \n",
       "33       NaN  \n",
       "34       NaN  \n",
       "...      ...  \n",
       "9087     NaN  \n",
       "9089     NaN  \n",
       "9090     NaN  \n",
       "9091     NaN  \n",
       "9092     NaN  \n",
       "\n",
       "[5788 rows x 2 columns]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine tokens with missing product values \n",
    "# to identify potential product mentions within the text \n",
    "# and use that to impute the missing values\n",
    "df[df['product'].isna()][['tokens', 'product']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb059b3",
   "metadata": {},
   "source": [
    "Defining our imputation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c81f05b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "product\n",
       "Apple     5568\n",
       "Google    2780\n",
       "NaN        722\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ======= [Product imputation pipeline] =======\n",
    "\n",
    "def impute_product(row):\n",
    "\n",
    "    '''\n",
    "        Uses the tokens to get keywords and map them to \n",
    "        either Google or Apple to impute missing product values\n",
    "    '''\n",
    "    \n",
    "    # If product is already assigned, keep it\n",
    "    if pd.notna(row['product']):\n",
    "        return row['product']\n",
    "    \n",
    "    # If product is NaN, check tokens for keywords\n",
    "    tokens = row['tokens']\n",
    "    \n",
    "    # Define keywords for each product\n",
    "    apple_keywords = ['iphone', 'ipad', 'apple', 'itunes', 'ipad2']\n",
    "    google_keywords = ['google', 'android', 'maps']\n",
    "    \n",
    "    # Check if any Apple keywords are in tokens\n",
    "    if any(keyword in tokens for keyword in apple_keywords):\n",
    "        return 'Apple'\n",
    "    \n",
    "    # Check if any Google keywords are in tokens\n",
    "    if any(keyword in tokens for keyword in google_keywords):\n",
    "        return 'Google'\n",
    "    \n",
    "    # If no keywords found, keep as NaN\n",
    "    return np.nan\n",
    "\n",
    "# Apply the imputation\n",
    "df['product'] = df.apply(impute_product, axis=1)\n",
    "\n",
    "# Check distribution after imputation\n",
    "df['product'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88277db",
   "metadata": {},
   "source": [
    "Excellent! Our keyword-based imputation reduced missing product values from over 5,000 down to approximately 700!! Now let's analyze the tokens from the remaining null entries to identify any additional patterns we can use for imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "5f5887ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[link, help, forward, doc, anonymous, account, techie, amp, ppl, help, jam, libya, sxsw]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>[link, edchat, musedchat, sxsw, sxswi, classical, newtwitter]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>[location, based, fast, fun, future, link, via, sxsw]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>[sxsw, wanna, buy, drink, 7pm, fado, 4th, link, join]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>[chilcott, sxsw, stand, talking, blogger, staff, late, win, competition, best, tweet, mentioning, shirt]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8932</th>\n",
       "      <td>[news, good, news, link, code, valid, 59p, infektd, sxsw, zlf]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8936</th>\n",
       "      <td>[client, news, release, quot, dope, melody, amp, heavy, bass, quot, amp, invades, sxsw, link]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8970</th>\n",
       "      <td>[5th, year, downloading, sxsw, music, torrent, link, free, legal, great, music]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9024</th>\n",
       "      <td>[way, looking, spanish, speaking, trend, scout, based, austin, link, sxsw]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9026</th>\n",
       "      <td>[true, story, rated, amy, ice, cream, star, quot, best, ice, cream, town, quot, link, sxsw]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>722 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                        tokens  \\\n",
       "51                    [link, help, forward, doc, anonymous, account, techie, amp, ppl, help, jam, libya, sxsw]   \n",
       "52                                               [link, edchat, musedchat, sxsw, sxswi, classical, newtwitter]   \n",
       "53                                                       [location, based, fast, fun, future, link, via, sxsw]   \n",
       "66                                                       [sxsw, wanna, buy, drink, 7pm, fado, 4th, link, join]   \n",
       "71    [chilcott, sxsw, stand, talking, blogger, staff, late, win, competition, best, tweet, mentioning, shirt]   \n",
       "...                                                                                                        ...   \n",
       "8932                                            [news, good, news, link, code, valid, 59p, infektd, sxsw, zlf]   \n",
       "8936             [client, news, release, quot, dope, melody, amp, heavy, bass, quot, amp, invades, sxsw, link]   \n",
       "8970                           [5th, year, downloading, sxsw, music, torrent, link, free, legal, great, music]   \n",
       "9024                                [way, looking, spanish, speaking, trend, scout, based, austin, link, sxsw]   \n",
       "9026               [true, story, rated, amy, ice, cream, star, quot, best, ice, cream, town, quot, link, sxsw]   \n",
       "\n",
       "     product  \n",
       "51       NaN  \n",
       "52       NaN  \n",
       "53       NaN  \n",
       "66       NaN  \n",
       "71       NaN  \n",
       "...      ...  \n",
       "8932     NaN  \n",
       "8936     NaN  \n",
       "8970     NaN  \n",
       "9024     NaN  \n",
       "9026     NaN  \n",
       "\n",
       "[722 rows x 2 columns]"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['product'].isna()][['tokens', 'product']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae77852",
   "metadata": {},
   "source": [
    "The remaining tokens offer no meaningful product indicators. We'll drop the 722 null entries to ensure dataset completeness and reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "0702d410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet          0\n",
       "product        0\n",
       "sentiment      0\n",
       "tokens         0\n",
       "clean_tweet    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna(subset=['product'])\n",
    "\n",
    "# Final completeness preview\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec9951b",
   "metadata": {},
   "source": [
    "## EXPLORATORY DATA ANALYSIS (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f83fdca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
